

# Version Parall√®le de Floyd-Warshall (MPI)

## 1. Description rapide

Ce dossier contient **ma version parall√®le** de l‚Äôalgorithme de Floyd-Warshall, √©crite en C++ avec **MPI**.
L‚Äôid√©e reste la m√™me que pour la version s√©quentielle : on veut les plus courts chemins entre tous les sommets.
La diff√©rence est que **la grande matrice est divis√©e en blocs**, et chaque processus traite les blocs dont il est responsable.

√Ä chaque it√©ration `k`, le bloc pivot est transmis aux autres processus (`MPI_Bcast` ou `MPI_Ibcast`), ce qui leur permet de mettre √† jour leurs blocs locaux.
C‚Äôest la m√©thode classique de **parall√©lisation 2D par blocs**, comme vue en TP.

üìå **R√©f√©rence consult√©e**
Pendant la r√©alisation, j‚Äôai aussi regard√© un document externe qui explique une approche proche (d√©coupage 2D, broadcasts, etc.).
Cela m‚Äôa aid√© √† organiser mon code.

> Asmita Gautam, *Parallel Floyd-Warshall Algorithm*, University at Buffalo, 2019.

---

## 2. Fichiers importants

* **`main_mpi.cpp`** ‚Äì programme principal
* **`ParallelFWBlocks.cpp` / `.hpp`** ‚Äì impl√©mentation de Floyd-Warshall par blocs
* **`Distribution.cpp`** ‚Äì r√©partition des blocs entre les processus
* **`Utils.cpp`** ‚Äì affichage, √©criture dans un fichier, etc.
* **`Makefile`** ‚Äì compilation automatique

---

## 3. Compilation

Se placer dans le dossier :

```
FLOYD_PARALLEL_RABAH/
```

Puis compiler :

```
make
```

Un ex√©cutable appara√Æt :

```
./main_mpi
```

---

## 4. Ex√©cution

Le programme attend un fichier contenant **une matrice d‚Äôadjacence**.

Commande g√©n√©rale :

```
mpirun -np <nb_processus> ./main_mpi <chemin_fichier_matrice>
```

Exemples :

```
mpirun -np 4 ./main_mpi ../../DATA/PetitExemple.dot
```

```
mpirun -np 9 ./main_mpi ../../DATA/exemplemassi.dot
```

> Les fichiers d‚Äôentr√©e se trouvent dans le dossier `DATA`.

---

## 5. Sortie du programme

Le programme :

* calcule la matrice des plus courts chemins,
* rassemble tout sur le **rang 0**,
* √©crit le r√©sultat dans :

```
DATA/matrice_finale_sortie_de_floyd_warshal.txt
```

Le rang 0 affiche aussi le **temps d‚Äôex√©cution MPI**.

---

## 6. Remarques utiles

* Si le nombre de processus n‚Äôest **pas un carr√©**, la grille est adapt√©e automatiquement (`MPI_Dims_create`).
* Si la taille de la matrice n‚Äôest **pas un multiple de la taille des blocs**, les endroits ‚Äúqui d√©passent‚Äù sont remplis avec **INF** (padding).
* Des **communications non bloquantes** sont utilis√©es pour √©viter que les processus attendent inutilement.

---

## 7. Exemple complet

```
make
mpirun -np 4 ./main_mpi ../../DATA/PetitExemple.dot
```

---

## 8. Nettoyage

Pour repartir propre :

```
make clean
```



