# Version Parall√®le de Floyd-Warshall (MPI)

## 1. Description rapide

Ce dossier contient **ma version parall√®le** de l‚Äôalgorithme de Floyd-Warshall.

Le programme ne lit pas directement une matrice d‚Äôadjacence :
 il lit un **graphe pond√©r√© au format Graphviz `.dot`**,
puis construit la matrice d‚Äôadjacence en m√©moire avant de lancer Floyd-Warshall par blocs.

La grande matrice est d√©coup√©e en **blocs**, et chaque processus s‚Äôoccupe de certains blocs.
√Ä chaque it√©ration, le bloc pivot est diffus√© aux autres processus pour mettre √† jour les distances.

üìå **R√©f√©rence consult√©e**
Asmita Gautam, *Parallel Floyd-Warshall Algorithm*, University at Buffalo, 2019.

---

## 2. Fichiers importants

* **`main_mpi.cpp`** ‚Äì point d‚Äôentr√©e MPI :
  lit le fichier `.dot`, construit la matrice d‚Äôadjacence, appelle `ParallelFloydWarshallBlocks`.
* **`ForGraphMPI.cpp / .hpp`** ‚Äì lecture du fichier DOT avec Graphviz (CGraph)
  ‚Üí transforme le graphe en matrice d‚Äôadjacence (non orient√©e, pond√©r√©e).
* **`ParallelFWBlocks.cpp / .hpp`** ‚Äì impl√©mentation de Floyd-Warshall par blocs (version parall√®le).
* **`Distribution.cpp / .hpp`** ‚Äì r√©partition des blocs entre les processus MPI.
* **`Utils.cpp / .hpp`** ‚Äì fonctions utilitaires (affichage, √©criture dans un fichier texte).
* **`Makefile`** ‚Äì script de compilation.

---

## 3. Compilation

Depuis le dossier :

```bash
cd FLOYD_PARALLEL_RABAH
make
```

√áa produit un ex√©cutable :

```bash
./main_mpi
```

---

## 4. Format du fichier d‚Äôentr√©e (`.dot`)

Le programme attend un **fichier DOT Graphviz** d√©crivant un graphe pond√©r√© non orient√©, par exemple :

```dot
graph graphe_pondere {
    node [shape=circle, style=filled, color=lightyellow, fontcolor=black];
    edge [color=black, fontcolor=blue];

    A [label="A"];
    B [label="B"];
    C [label="C"];

    A -- B [label="5", weight=5];
    A -- C [label="2", weight=2];
    B -- C [label="3", weight=3];
}
```

`ForGraphMPI.cpp` parcourt ce fichier, num√©rote les sommets (0, 1, 2, ‚Ä¶) et construit une matrice d‚Äôadjacence `nb_nodes √ó nb_nodes` avec les poids, puis on applique Floyd-Warshall sur cette matrice.

Les fichiers `.dot` d‚Äôexemple sont dans le dossier :

```bash
../DATA
```

---

## 5. Ex√©cution

Commande g√©n√©rale :

```bash
mpirun -np <nb_processus> ./main_mpi <chemin_fichier_dot>
```

Exemple :

```bash
mpirun -np 4 ./main_mpi ../DATA/PetitExemple.dot
```

Ou avec un autre graphe :

```bash
mpirun -np 4 ./main_mpi ../DATA/Resulat_sequence_by_premier_algo.dot
```

---

## 6. Sortie du programme

√Ä la fin du calcul, le **rang 0** :

* r√©cup√®re la matrice finale des plus courts chemins,
* l‚Äô√©crit dans un fichier texte :

```bash
../DATA/matrice_finale_sortie_de_floyd_warshal.txt
```

Ce fichier sera ensuite utilis√© comme entr√©e pour l‚Äôalgorithme de PAM.

Le rang 0 affiche aussi le **temps d‚Äôex√©cution de la partie parall√®le** (entre les deux `MPI_Barrier` dans `main_mpi.cpp`).

---



## 7. Nettoyage

Pour supprimer les fichiers objets et recompiler proprement :

```bash
make clean
```



## Parall√©lisation de l‚Äôalgorithme de Floyd‚ÄìWarshall

### Principe g√©n√©ral

L‚Äôalgorithme de Floyd‚ÄìWarshall calcule les plus courts chemins entre tous les couples de sommets en appliquant la relation :

                        D[i][j] = min(D[i][j], D[i][k] + D[k][j])


Pour rendre ce calcul exploitable sur des graphes de taille plus importante, j‚Äôai utilis√© une version **par blocs** parall√©lis√©e avec **MPI**.
La matrice des distances est d√©coup√©e en blocs (b \times b), et ces blocs sont r√©partis sur une **grille 2D de processus**. Chaque processus ne stocke et ne met √† jour que les blocs dont il est responsable.

Cette approche reprend le sch√©ma pr√©sent√© dans le sujet (d√©coupage en blocs, grille de processus, diffusion du pivot) et la structure d√©crite dans le document d‚ÄôAsmita Gautam (*Parallel Floyd‚ÄìWarshall Algorithm*, 2019).

### Distribution initiale

Au d√©part, la matrice d‚Äôadjacence est connue int√©gralement sur le rang 0.
Elle est ensuite distribu√©e bloc par bloc : pour chaque bloc ((i,j)), une fonction de r√©partition (`ownerOf`) indique sur quel processus il doit r√©sider. Ce processus initialise son bloc √† partir de la matrrice d‚Äôadjacence (poids de l‚Äôar√™te, 0 sur la diagonale, ‚Äúpas de chemin direct‚Äù sinon).

Ainsi, chaque rang MPI dispose d‚Äôun sous-ensemble de blocs, pas forc√©ment contigus, et la matrice globale est implicitement r√©partie sur toute la grille.

### D√©roulement d‚Äôune it√©ration parall√®le

Pour chaque √©tape (k) (bloc diagonal ((k,k))) :

1. **Bloc pivot**
   Le processus qui poss√®de le bloc ((k,k)) applique localement Floyd‚ÄìWarshall **√† l‚Äôint√©rieur de ce bloc** (`fw_block`).
   Le bloc pivot mis √† jour est ensuite diffus√© √† tous les processus par `MPI_Bcast`.

2. **Mise √† jour de la ligne (k)**
   Pour chaque bloc ((k, j)) sur la m√™me ligne que le pivot :

   * le processus propri√©taire met √† jour ce bloc avec `fw_row`, en utilisant le pivot,
   * puis le bloc mis √† jour est diffus√© √† tous via `MPI_Ibcast`.
     Chaque rang re√ßoit ces blocs mais n‚Äôutilise que ceux dont il a besoin pour ses blocs internes.

3. **Mise √† jour de la colonne (k)**
   De mani√®re sym√©trique, pour chaque bloc ((i, k)) sur la m√™me colonne :

   * le propri√©taire applique `fw_col`,
   * puis diffuse le r√©sultat √† tous les processus avec `MPI_Ibcast`.

4. **Mise √† jour des blocs internes**
   Une fois les blocs de la ligne (k) et de la colonne (k) disponibles, chaque processus met √† jour ses **blocs internes** ((i,j)) (ni sur la ligne (k), ni sur la colonne (k)) √† l‚Äôaide de `fw_inner`, en combinant :

   * le bloc ((i,k)) re√ßu dans `colBlocks[i]`,
   * le bloc ((k,j)) re√ßu dans `rowBlocks[j]`.

L‚Äôusage de `MPI_Ibcast` permet de recouvrir une partie des communications avec les calculs locaux : pendant que certains blocs sont en train d‚Äô√™tre diffus√©s, les processus peuvent d√©j√† commencer √† traiter d‚Äôautres blocs.

### Rassemblement du r√©sultat

√Ä la fin des it√©rations, chaque processus poss√®de la version finale des blocs dont il est responsable.
Le rang 0 r√©cup√®re ces blocs un par un (en utilisant `MPI_Send` / `MPI_Recv`) et reconstruit la matrice compl√®te des distances (n * n).
Cette matrice est ensuite utilis√©e comme entr√©e de l‚Äôalgorithme PAM pour la phase de clustering.


